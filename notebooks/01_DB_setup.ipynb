{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector\n",
    "from snowflake.snowpark.session import Session\n",
    "from snowflake.snowpark.types import (\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    "    DateType,\n",
    "    BooleanType,\n",
    "    DecimalType,\n",
    "    FloatType,\n",
    "    TimestampType,\n",
    "    VariantType,\n",
    "    ArrayType,\n",
    ")\n",
    "\n",
    "import toml\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create needed datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(os.path.dirname(os.getcwd()), \"data\")\n",
    "df = pd.read_csv(os.path.join(data_path, \"heart.csv\"))\n",
    "\n",
    "# Split indices\n",
    "indices = np.random.permutation(len(df))\n",
    "split1 = int(len(df) / 3)\n",
    "split2 = int(2 * len(df) / 3)\n",
    "\n",
    "# Create the three datasets\n",
    "df1 = df.iloc[indices[:split1]]\n",
    "df2 = df.iloc[indices[split1:split2]]\n",
    "df3 = df.iloc[indices[split2:]]\n",
    "\n",
    "df1.to_csv(os.path.join(data_path, \"dataset1.csv\"), index=False)\n",
    "df2.to_csv(os.path.join(data_path, \"dataset2.csv\"), index=False)\n",
    "df3.to_csv(os.path.join(data_path, \"dataset3.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Connect to Snowflake and create database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use snowflake.connector to connect to Snowflake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = toml.load(\"../config.toml\")\n",
    "connection_parameters = config[\"snowflake_connection\"]\n",
    "\n",
    "session = Session.builder.configs(connection_parameters).create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I will execute SQL commands to create a database and a schema in Snowflake.\n",
    "\n",
    "I will not create a warehouse because I already have one, but the commented command can be used if a new warehouse is needed in the future.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='Database HEART_DB successfully created.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sql(\"CREATE OR REPLACE DATABASE HEART_DB\").collect()\n",
    "# session.sql(\"CREATE OR REPLACE WAREHOUSE compute_wh WITH WAREHOUSE_SIZE='X-SMALL'\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the necessary database and schema, it is essential to set the current schema for subsequent SQL operations. The following command sets the schema to `PUBLIC`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='Statement executed successfully.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sql(\"USE SCHEMA PUBLIC;\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the current settings of the Snowflake session, we can execute the following SQL command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(CURRENT_WAREHOUSE()='COMPUTE_WH', CURRENT_DATABASE()='HEART_DB', CURRENT_SCHEMA()='PUBLIC', CURRENT_USER()='JOANABAIAO', CURRENT_ROLE()='ACCOUNTADMIN')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sql(\n",
    "    \"SELECT current_warehouse(), current_database(), current_schema(), current_user(), current_role()\"\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create stages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a stage in Snowflake is an important part of managing data in machine learning workflows. A stage is essentially a location where data files can be stored before they are loaded into a table or processed.\n",
    "\n",
    "I will create the following stages:\n",
    "\n",
    "- **LOAD_DATA_STAGE**: To ingest data into Snowflake.\n",
    "- **MODEL_STAGE**: To store machine learning models that are generated during the project.\n",
    "- **FUNCTION_STAGE**: To store Python functions\n",
    "- **PACKAGE_STAGE**: To store any libraries not available in the Anaconda repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='Stage area PACKAGE_STAGE successfully created.')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_data_stage = \"LOAD_DATA_STAGE\"\n",
    "model_stage = \"MODEL_STAGE\"\n",
    "function_stage = \"FUNCTION_STAGE\"\n",
    "package_stage = \"PACKAGE_STAGE\"\n",
    "\n",
    "session.sql(f\"CREATE OR REPLACE STAGE {load_data_stage}\").collect()\n",
    "session.sql(f\"CREATE OR REPLACE STAGE {model_stage}\").collect()\n",
    "session.sql(f\"CREATE OR REPLACE STAGE {function_stage}\").collect()\n",
    "session.sql(f\"CREATE OR REPLACE STAGE {package_stage}\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will also create a sequence for generating unique model IDs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='Sequence MODEL_SEQ successfully created.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sql(\n",
    "    \"CREATE OR REPLACE SEQUENCE MODEL_SEQ START WITH 1 INCREMENT BY 1;\"\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Load data to stage \"LOAD_DATA_STAGE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PutResult(source='dataset3.csv', target='dataset3.csv.gz', source_size=12723, target_size=2832, source_compression='NONE', target_compression='GZIP', status='UPLOADED', message='')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = os.path.join(os.path.dirname(os.getcwd()), \"data\")\n",
    "file_path1 = os.path.join(data_path, \"dataset1.csv\")\n",
    "file_path2 = os.path.join(data_path, \"dataset2.csv\")\n",
    "file_path3 = os.path.join(data_path, \"dataset3.csv\")\n",
    "\n",
    "session.file.put(file_path1, f\"@{load_data_stage}\")\n",
    "session.file.put(file_path2, f\"@{load_data_stage}\")\n",
    "session.file.put(file_path3, f\"@{load_data_stage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm that the file is in the \"LOAD_DATA_STAGE\" stage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(name='load_data_stage/dataset1.csv.gz', size=2880, md5='111cdba7ac2932838eca831a839746b5', last_modified='Wed, 22 Jan 2025 11:33:36 GMT')\n",
      "Row(name='load_data_stage/dataset2.csv.gz', size=2976, md5='cb5026952be0389eee7588b433f696c7', last_modified='Wed, 22 Jan 2025 11:33:37 GMT')\n",
      "Row(name='load_data_stage/dataset3.csv.gz', size=2832, md5='ba71e2057c5e5cec0c5d153034eeeca6', last_modified='Wed, 22 Jan 2025 11:33:37 GMT')\n"
     ]
    }
   ],
   "source": [
    "files = session.sql(f\"LIST @{load_data_stage}\").collect()\n",
    "for file in files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create needed tables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will create several internal tables within Snowflake to manage and store key information related to the heart attack prediction models and patient data. These tables will be integral for storing model metadata, performance metrics, and patient health data, which can then be used for analysis and model evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Create the `MODEL_TRAINING_INFO` table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table will store details about each model generated, such as model name, training data, and the scoring results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_log = StructType(\n",
    "    [\n",
    "        StructField(\"training_date\", TimestampType()),\n",
    "        StructField(\"model_id\", StringType()),\n",
    "        StructField(\"model_name\", StringType()),\n",
    "        StructField(\"optimization\", BooleanType()),\n",
    "        StructField(\"training_table\", StringType()),\n",
    "        StructField(\"feature_columns\", ArrayType(StringType())),\n",
    "        StructField(\"accuracy\", FloatType()),\n",
    "        StructField(\"precision\", FloatType()),\n",
    "        StructField(\"recall\", FloatType()),\n",
    "        StructField(\"f1_score\", FloatType()),\n",
    "        StructField(\"auc_roc\", FloatType()),\n",
    "        StructField(\"TN\", IntegerType()),\n",
    "        StructField(\"FP\", IntegerType()),\n",
    "        StructField(\"FN\", IntegerType()),\n",
    "        StructField(\"TP\", IntegerType()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "df_log = session.create_dataframe([], schema=schema_log)\n",
    "df_log.write.mode(\"overwrite\").save_as_table(\"MODEL_TRAINING_INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Create the `INFERENCE_RESULTS` table\n",
    "\n",
    "This table will be used to store the details of each prediction made by the deployed models. It will include information like the model used, the input data for inference, and the prediction results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_inference = StructType(\n",
    "    [\n",
    "        StructField(\"inference_date\", TimestampType()),\n",
    "        StructField(\"model_id\", StringType()),\n",
    "        StructField(\"training_table\", StringType()),\n",
    "        StructField(\"test_table\", StringType()),\n",
    "        #StructField(\"predictions_table\", StringType()),\n",
    "        StructField(\"accuracy\", FloatType()),\n",
    "        StructField(\"precision\", FloatType()),\n",
    "        StructField(\"recall\", FloatType()),\n",
    "        StructField(\"f1_score\", FloatType()),\n",
    "        StructField(\"auc_roc\", FloatType()),\n",
    "        StructField(\"TN\", IntegerType()),\n",
    "        StructField(\"FP\", IntegerType()),\n",
    "        StructField(\"FN\", IntegerType()),\n",
    "        StructField(\"TP\", IntegerType()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "df_inference = session.create_dataframe([], schema=schema_inference)\n",
    "df_inference.write.mode(\"overwrite\").save_as_table(\"INFERENCE_RESULTS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Create the `MODEL_CATALOG` table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a table to store the names of the available models. The Streamlit app will query this table to present the list of model names to the user, allowing them to choose a model for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_model = StructType([StructField(\"model_name\", StringType(), True)])\n",
    "\n",
    "# Create a df with model names\n",
    "model_names = [\n",
    "    [\"Random Forest\"],\n",
    "    [\"XGBoost\"],\n",
    "    [\"K-Nearest Neighbors\"],\n",
    "    [\"Support Vector Machine\"]\n",
    "]\n",
    "df_models_table = session.create_dataframe(model_names, schema=schema_model)\n",
    "\n",
    "# Write the df to the models table\n",
    "df_models_table.write.mode(\"overwrite\").save_as_table(\"MODEL_CATALOG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Create tables with the patients data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1. Test data upload process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before automating the process, we first load a sample table manually to ensure everything works correctly.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Schema Definition:** I will define the schema to ensure that the data is correctly parsed when loading the CSV file into Snowflake.\n",
    "2. **Load the Data:** We load the data from the staging area into a Snowflake DataFrame using the schema.\n",
    "3. **Copy Data Into Snowflake Table:** Once the data is loaded into a Snowflake DataFrame, we copy it into a Snowflake table. This process ensures that the data is available in the correct table for further processing or analysis.\n",
    "4. **Check Data in Table:** To ensure the data is present in the Snowflake table, query it and display a sample. The `session.table()` command fetches the data from the specified table, and result_df.show(5) displays the first 5 rows of the table to verify that the data has been successfully loaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"age\", IntegerType()),\n",
    "        StructField(\"sex\", IntegerType()),\n",
    "        StructField(\"cp\", IntegerType()),\n",
    "        StructField(\"trestbps\", DecimalType()),\n",
    "        StructField(\"chol\", IntegerType()),\n",
    "        StructField(\"fbs\", DecimalType()),\n",
    "        StructField(\"restecg\", DecimalType()),\n",
    "        StructField(\"thalach\", DecimalType()),\n",
    "        StructField(\"exang\", DecimalType()),\n",
    "        StructField(\"oldpeak\", DecimalType()),\n",
    "        StructField(\"slope\", DecimalType()),\n",
    "        StructField(\"ca\", DecimalType()),\n",
    "        StructField(\"thal\", IntegerType()),\n",
    "        StructField(\"target\", IntegerType()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGE</th>\n",
       "      <th>SEX</th>\n",
       "      <th>CP</th>\n",
       "      <th>TRESTBPS</th>\n",
       "      <th>CHOL</th>\n",
       "      <th>FBS</th>\n",
       "      <th>RESTECG</th>\n",
       "      <th>THALACH</th>\n",
       "      <th>EXANG</th>\n",
       "      <th>OLDPEAK</th>\n",
       "      <th>SLOPE</th>\n",
       "      <th>CA</th>\n",
       "      <th>THAL</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>263</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>140</td>\n",
       "      <td>313</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>290</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>153</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>126</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>318</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AGE  SEX  CP  TRESTBPS  CHOL  FBS  RESTECG  THALACH  EXANG  OLDPEAK  SLOPE  \\\n",
       "0   44    1   1       120   263    0        1      173      0        0      2   \n",
       "1   64    0   2       140   313    0        1      133      0        0      2   \n",
       "2   44    1   0       112   290    0        0      153      0        0      2   \n",
       "3   57    1   2       150   126    1        1      173      0        0      2   \n",
       "4   58    1   0       114   318    0        2      140      0        4      0   \n",
       "\n",
       "   CA  THAL  TARGET  \n",
       "0   0     3       1  \n",
       "1   0     3       1  \n",
       "2   1     2       0  \n",
       "3   1     3       1  \n",
       "4   3     1       0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define variables for the destination table and file name\n",
    "file_name = \"dataset1.csv\"  # Path to the CSV file in the staging area\n",
    "table_name = \"TEST_TABLE\"\n",
    "\n",
    "# Define snowflake dataframe\n",
    "df_heart = (\n",
    "    session.read.schema(schema)\n",
    "    .options({\"FIELD_DELIMITER\": \",\", \"SKIP_HEADER\": 1})\n",
    "    .csv(f\"@{load_data_stage}/{file_name}\")\n",
    ")\n",
    "# df_heart.show(5)\n",
    "\n",
    "# Copy data into table\n",
    "copied_into_result = df_heart.copy_into_table(\n",
    "    table_name, force=True, on_error=\"CONTINUE\"\n",
    ")\n",
    "\n",
    "# Check data in table\n",
    "df_heart_test = session.table(table_name)\n",
    "df_heart_test.limit(5).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2. Automate the Process with a Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the manual process has been validated, I will create a reusable function, copy_into(), that can be used in the Streamlit app to load staged data into Snowflake tables. This function will allow us to load data automatically based on the CSV file provided by the user.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_to_table(session: Session, file_name: str, table_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Loads CSV data from the staging area into a Snowflake table.\n",
    "    \"\"\"\n",
    "\n",
    "    # Import required modules inside the function to ensure portability\n",
    "    from snowflake.snowpark.types import (\n",
    "        StructType,\n",
    "        StructField,\n",
    "        IntegerType,\n",
    "        DecimalType,\n",
    "    )\n",
    "\n",
    "    # Define schema for patient heart data\n",
    "    schema_heart = StructType(\n",
    "        [\n",
    "            StructField(\"age\", IntegerType()),\n",
    "            StructField(\"sex\", IntegerType()),\n",
    "            StructField(\"cp\", IntegerType()),\n",
    "            StructField(\"trestbps\", DecimalType(10, 2)),\n",
    "            StructField(\"chol\", IntegerType()),\n",
    "            StructField(\"fbs\", DecimalType(10, 2)),\n",
    "            StructField(\"restecg\", DecimalType(10, 2)),\n",
    "            StructField(\"thalach\", DecimalType(10, 2)),\n",
    "            StructField(\"exang\", DecimalType(10, 2)),\n",
    "            StructField(\"oldpeak\", DecimalType(10, 2)),\n",
    "            StructField(\"slope\", DecimalType(10, 2)),\n",
    "            StructField(\"ca\", DecimalType(10, 2)),\n",
    "            StructField(\"thal\", IntegerType()),\n",
    "            StructField(\"target\", IntegerType()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Read the CSV file and load it into the specified table\n",
    "        session.read.option(\"FIELD_DELIMITER\", \",\").option(\"SKIP_HEADER\", 1).option(\n",
    "            \"ON_ERROR\", \"CONTINUE\"\n",
    "        ).schema(schema_heart).csv(file_name).copy_into_table(table_name)\n",
    "\n",
    "        return f\"{file_name} data successfully copied into table '{table_name}'\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error occurred while copying data into table '{table_name}': {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if the function is working correctly and verify if table was created\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGE</th>\n",
       "      <th>SEX</th>\n",
       "      <th>CP</th>\n",
       "      <th>TRESTBPS</th>\n",
       "      <th>CHOL</th>\n",
       "      <th>FBS</th>\n",
       "      <th>RESTECG</th>\n",
       "      <th>THALACH</th>\n",
       "      <th>EXANG</th>\n",
       "      <th>OLDPEAK</th>\n",
       "      <th>SLOPE</th>\n",
       "      <th>CA</th>\n",
       "      <th>THAL</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120.0</td>\n",
       "      <td>263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>140.0</td>\n",
       "      <td>313</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>290</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150.0</td>\n",
       "      <td>126</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>318</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AGE  SEX  CP  TRESTBPS  CHOL  FBS  RESTECG  THALACH  EXANG  OLDPEAK  SLOPE  \\\n",
       "0   44    1   1     120.0   263  0.0      1.0    173.0    0.0      0.0    2.0   \n",
       "1   64    0   2     140.0   313  0.0      1.0    133.0    0.0      0.2    2.0   \n",
       "2   44    1   0     112.0   290  0.0      0.0    153.0    0.0      0.0    2.0   \n",
       "3   57    1   2     150.0   126  1.0      1.0    173.0    0.0      0.2    2.0   \n",
       "4   58    1   0     114.0   318  0.0      2.0    140.0    0.0      4.4    0.0   \n",
       "\n",
       "    CA  THAL  TARGET  \n",
       "0  0.0     3       1  \n",
       "1  0.0     3       1  \n",
       "2  1.0     2       0  \n",
       "3  1.0     3       1  \n",
       "4  3.0     1       0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define variables for the destination table and file name\n",
    "file_name = \"@load_data_stage/dataset1.csv\"  # Path to the CSV file in the staging area\n",
    "table_name = \"TEST_TABLE2\"\n",
    "\n",
    "load_data_to_table(session, file_name, table_name)\n",
    "\n",
    "df_heart_test = session.table(table_name)\n",
    "df_heart_test.limit(5).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3. Register the copy_into() function as a stored procedure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimize the data loading process, we register the _copy_into()_ function as a **stored procedure** in Snowflake. This allows the function to be executed directly within Snowflake, improving performance, centralizing logic, and making the operation reusable across different platforms (like Streamlit).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.snowpark.stored_procedure.StoredProcedure at 0x10eea4c50>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sproc.register(\n",
    "    func=load_data_to_table,\n",
    "    name=\"load_data_to_table\",\n",
    "    packages=[\"snowflake-snowpark-python\"],\n",
    "    is_permanent=True,\n",
    "    stage_location=f\"@{function_stage}\",\n",
    "    replace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To execute the stored procedure using Snowpark, we can use the `session.call` method.\n",
    "\n",
    "The `session.call()` method triggers the stored procedure, passing the file path and target table name as parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@LOAD_DATA_STAGE/dataset3.csv data successfully copied into table 'DATA_TABLE_3'\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.call(\"load_data_to_table\", f\"@{load_data_stage}/dataset1.csv\", \"DATA_TABLE_1\")\n",
    "session.call(\"load_data_to_table\", f\"@{load_data_stage}/dataset2.csv\", \"DATA_TABLE_2\")\n",
    "session.call(\"load_data_to_table\", f\"@{load_data_stage}/dataset3.csv\", \"DATA_TABLE_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='TEST_TABLE2 successfully dropped.')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sql(f\"DROP TABLE IF EXISTS TEST_TABLE\").collect()\n",
    "session.sql(f\"DROP TABLE IF EXISTS TEST_TABLE2\").collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snowpark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
